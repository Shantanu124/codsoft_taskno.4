{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline    \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import precision_score,confusion_matrix, recall_score, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"...\\spam.csv\",encoding='latin-1')#put the path of the dataset at the place of ... \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"]\n",
    "data = data.drop(data[to_drop], axis=1)\n",
    "data.rename(columns = {\"v1\":\"Target\", \"v2\":\"Text\"}, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= [\"#E1F16B\", \"#E598D8\"] \n",
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.countplot(x= data[\"Target\"], palette= cols)\n",
    "fg.set_title(\"Count Plot of Classes\", color=\"#58508d\")\n",
    "fg.set_xlabel(\"Classes\", color=\"#58508d\")\n",
    "fg.set_ylabel(\"Number of Data points\", color=\"#58508d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"No_of_Characters\"] = data[\"Text\"].apply(len)\n",
    "data[\"No_of_Words\"]=data.apply(lambda row: nltk.word_tokenize(row[\"Text\"]), axis=1).apply(len)\n",
    "data[\"No_of_sentence\"]=data.apply(lambda row: nltk.sent_tokenize(row[\"Text\"]), axis=1).apply(len)\n",
    "\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data=data, hue=\"Target\",palette=cols)\n",
    "plt.show(fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"No_of_Characters\"]<350)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "fg = sns.pairplot(data=data, hue=\"Target\",palette=cols)\n",
    "plt.show(fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts:\\033[0m\",*data[\"Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean(Text):\n",
    "    sms = re.sub('[^a-zA-Z]', ' ', Text) #Replacing all non-alphabetic characters with a space\n",
    "    sms = sms.lower() #converting to lowecase\n",
    "    sms = sms.split()\n",
    "    sms = ' '.join(sms)\n",
    "    return sms\n",
    "\n",
    "data[\"Clean_Text\"] = data[\"Text\"].apply(Clean)\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after cleaning:\\033[0m\",*data[\"Clean_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tokenize_Text\"]=data.apply(lambda row: nltk.word_tokenize(row[\"Clean_Text\"]), axis=1)\n",
    "\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after Tokenizing:\\033[0m\",*data[\"Tokenize_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "data[\"Nostopword_Text\"] = data[\"Tokenize_Text\"].apply(remove_stopwords)\n",
    "\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after removing the stopwords:\\033[0m\",*data[\"Nostopword_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_word(text):\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    return lemmas\n",
    "\n",
    "data[\"Lemmatized_Text\"] = data[\"Nostopword_Text\"].apply(lemmatize_word)\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts after lemitization:\\033[0m\",*data[\"Lemmatized_Text\"][:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= []\n",
    "for i in data[\"Lemmatized_Text\"]:\n",
    "    msg = ' '.join([row for row in i])\n",
    "    corpus.append(msg)   \n",
    "corpus[:5]\n",
    "print(\"\\033[1m\\u001b[45;1m The First 5 lines in corpus :\\033[0m\",*corpus[:5], sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data[\"Target\"] = label_encoder.fit_transform(data[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Target\"] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [MultinomialNB(), \n",
    "               RandomForestClassifier(),\n",
    "               KNeighborsClassifier(), \n",
    "               SVC()]\n",
    "for cls in classifiers:\n",
    "    cls.fit(X_train, y_train)\n",
    "pipe_dict = {0: \"NaiveBayes\", 1: \"RandomForest\", 2: \"KNeighbours\",3: \"SVC\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(classifiers):\n",
    "    cv_score = cross_val_score(model, X_train,y_train,scoring=\"accuracy\", cv=10)\n",
    "    print(\"%s: %f \" % (pipe_dict[i], cv_score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision =[]\n",
    "recall =[]\n",
    "f1_score = []\n",
    "trainset_accuracy = []\n",
    "testset_accuracy = []\n",
    "\n",
    "for i in classifiers:\n",
    "    pred_train = i.predict(X_train)\n",
    "    pred_test = i.predict(X_test)\n",
    "    prec = metrics.precision_score(y_test, pred_test)\n",
    "    recal = metrics.recall_score(y_test, pred_test)\n",
    "    f1_s = metrics.f1_score(y_test, pred_test)\n",
    "    train_accuracy = model.score(X_train,y_train)\n",
    "    test_accuracy = model.score(X_test,y_test)\n",
    "    precision.append(prec)\n",
    "    recall.append(recal)\n",
    "    f1_score.append(f1_s)\n",
    "    trainset_accuracy.append(train_accuracy)\n",
    "    testset_accuracy.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Precision':precision,\n",
    "'Recall':recall,\n",
    "'F1score':f1_score,\n",
    "'Accuracy on Testset':testset_accuracy,\n",
    "'Accuracy on Trainset':trainset_accuracy}\n",
    "Results = pd.DataFrame(data, index =[\"NaiveBayes\", \"RandomForest\", \"KNeighbours\",\"SVC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap2 = ListedColormap([\"#E2CCFF\",\"#E598D8\"])\n",
    "Results.style.background_gradient(cmap=cmap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=accuracy_score(pred_test,y_train)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
